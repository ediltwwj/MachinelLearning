多项式回归(PolyRegression) : 线性回归的局限性是只能应用于存在线性关系的数据中，但是在实际生活中，很多数据之间是非线性关系，
            虽然也可以用线性回归拟合非线性回归，但是效果将会很差，这时候就需要对线性回归模型进行改进，使之能够拟合非线性数据。

regressionComparsion : 模拟数据，比较线性回归和非线性回归的拟合效果
sklPoly : 在scikit-learn中使用多项式回归c测试模拟数据
usePipeline : 可以将许多算法模型串联起来，比如将特征提取、归一化、分类组织在一起形成一个典型的机器学习问题工作流
fitting : 过拟合和欠拟合
studyLine ： 学习曲线，反映随时数据量增大，训练模型的泛化能力变化


    模型在训练数据集上误差很小，导致发生过拟合而不自知，因此需要分为训练集和测试集
    使用测试数据集和训练数据集去拟合存在一个问题，针对特定测试数据集发生过拟合。
    因此，为解决此问题，可以将数据集分为三部分，训练数据集，验证数据集，测试数据集。
    训练数据集和验证数据集都参与模型的创建，验证数据集是调整超参数使用的数据集，
    而测试数据集作为衡量最终模型性能的数据集。
    分三部分也存在一个问题，即验证数据集随机，会有极值影响。可以使用交叉验证解决

    交叉验证即把训练数据集分为几份，选取一部分作为训练数据集，剩余的当做验证数据集，交叉验证。
    K个模型的均值作为结果进行调参。

crossValidation ： 交叉验证
biasVarianceTradeOff : 偏差方差权衡
    模型误差 = 偏差 + 方差 + 不可避免的误差
    导致偏差的主要原因 ： 对问题本身的假设不正确，如非线性数据使用线性回归
    导致方差的主要原因 ： 数据的一点点扰动都会较大地影响模型。通常原因，使用
        的模型太复杂，如高阶多项式回归。

        有一些算法天生是高方差算法，如KNN
        非参数学习通常都是高方差算法。因为不对数据进行任何假设
        有一些算法天生是高偏差算法，如线性回归
        参数学习通常都是高偏差算法。因为对数据具有极强的假设

        大多数算法具有相应的参数，可以调整偏差和方差
        如KNN中的K
        如线性回归中实用多项式回归

        偏差和方差通常都是矛盾的。
        降低偏差，会提高方差。
        降低方差，会提高偏差。

        解决高方差的通常手段：
        1、降低模型复杂度
        2、减少数据维度，降噪
        3、增加样本数
        4、使用验证集
        5、模型正则化

regularization : 模型正则化
    限制参数的大小

ridgeRegression : 岭回归
    目标 : 使得J函数值尽可能小
