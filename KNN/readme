K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。
该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。
              注意考虑距离。可以解决平票的情况。距离可以是欧拉距离或者曼哈顿距离，明可夫斯基距离。metric可以改变距离参数。

kNN是一个不需要训练过程的算法
可以解决多分类任务
也可以解决回归任务(KNeighborsRegressor)

kNN效率低下，m个样本，n个特征， 一个数据O(m*n)
优化,使用树结构
高度数据相关，对outer值敏感
预测结果不具有可解释性
维数灾难，随着维度的增加，“看似近似”的两个点之间的距离越来越大，解决方法降维

kNN 自定义kNN算法实现
modelSplit 自定义将数据集分割为训练集和测试集的实现
knnProc kNN算法原理的模拟过程

数据归一化处理 : 将所有数据映射到统一尺度
    最值归一化 ： 把所有数据归一化到0-1之间 X(scale) = X - X(min) / X(max) - X(min)
        适用于分布有明显边界的情况，受outlier影响较大

    均值方差归一化 ： 把所有数据归一到均值为0方差为1的分布中 X(scale) = X - X(mean) / S
        适用于数据分布没有明显的边界，有可能存在极端数据值，一般采用均值方差归一化

对测试数据集如何归一化？
(X_test - mean_train) / std_train
    因为真实环境中很有可能无法得到所有测试数据的均值和方差
    要保存训练数据集得到的均值和方差
    scikit-learn中使用Scalar进行归一化

kNN : kNN算法的自定义实现
kNNProc : kNN算法过程模拟
modelSplit : 将数据集拆分为训练集和测试集
preprocessing : 自定义均值方差归一化的封装
useKnnClassifier : 简单使用scikit-learn封装的kNN算法解决分类问题
useKnnRegressor : 简单使用scikit-learn封装的kNN算法解决回归问题