### 支撑向量机（SVM）
#### 1、什么是支撑向量机？
&emsp;&emsp;支撑向量机(Support Vector Machine，SVM)的基本模型是在特征空间上
找到最佳的分离超平面使得训练集上正负样本间隔最大。SVM是用来解决二分类问题的有监督学习算法，
在引入了核方法之后SVM也可以用来解决非线性问题。  
&emsp;&emsp;一般SVM有以下三种：  
&emsp;&emsp;（1）硬间隔支持向量机（线性可分支持向量机）：当训练数据线性可分时，
可通过硬间隔最大化学得一个线性可分支持向量机。  
&emsp;&emsp;（2）软间隔支持向量机：当训练数据近似线性可分时，可通过软间隔最大化学得一个线性支持向量机。  
&emsp;&emsp;（3）非线性支持向量机：当训练数据线性不可分时，可通过核方法以及软间隔最大化学得一个非线性支持向量机。  
&emsp;&emsp;SVM尝试寻找一个最优的决策边界。

#### 2、scikit-learn中的SVM
&emsp;&emsp;因为涉及到距离，因此需要保证各个维度的数据处于同一个量级，需进行数据标准化操作。   
&emsp;&emsp;参考sklearnSvm， sklearnSvm2文件。

#### 3、什么是核函数？
&emsp;&emsp;核函数包括线性核函数、多项式核函数、高斯核函数等，其中高斯核函数最常用，
可以将数据映射到无穷维，也叫做径向基函数（Radial Basis Function 简称 RBF）。  
&emsp;&emsp;机器学习中，对于线性可分的情况研究的比较透彻，但是很多情况是我们希望我们的模型学习非线性的模型。
通常的做法就是选择一个函数ϕ(x)将x映射到另一个空间中，即通过某非线性变换ϕ(x) ，将输入空间映射到高维特征空间。关键是如何选择ϕ(x)。  
&emsp;&emsp;假设X是输入空间，H是X映射到的高维特征空间，存在一个映射ϕ使得X中的点x能够计算得到H空间中的点h：   h=ϕ(x)
对于所有的X中的点都成立。x，z是X空间中的点，如果支持向量机的求解只用到内积运算，而在低维输入空间又存在某个函数k(x,z)满足条件：
k(x,z)=ϕ(x)⋅ϕ(z)    则称k为核函数，而ϕ为映射函数。  
&emsp;&emsp;核函数带来的好处很明显，如果先要映射到高维空间然后进行模型学习，
计算量远远大于在低维空间中直接直接采用核函数计算。还可以节省存储空间。

#### 4、SVM思路解决回归问题
&emsp;&emsp;与分类问题相反，回归问题要求落在margin之间的样本数越多越好

